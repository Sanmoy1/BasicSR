{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XydLNWu_d4Fg",
        "outputId": "86654a92-03ef-489a-96bf-92382a0217ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 0.2476\n",
            "Epoch [2/20], Loss: 0.2107\n",
            "Epoch [3/20], Loss: 0.1323\n",
            "Epoch [4/20], Loss: 0.0204\n",
            "Epoch [5/20], Loss: 0.1381\n",
            "Epoch [6/20], Loss: 0.0140\n",
            "Epoch [7/20], Loss: 0.0381\n",
            "Epoch [8/20], Loss: 0.0512\n",
            "Epoch [9/20], Loss: 0.0488\n",
            "Epoch [10/20], Loss: 0.0362\n",
            "Epoch [11/20], Loss: 0.0206\n",
            "Epoch [12/20], Loss: 0.0117\n",
            "Epoch [13/20], Loss: 0.0169\n",
            "Epoch [14/20], Loss: 0.0281\n",
            "Epoch [15/20], Loss: 0.0276\n",
            "Epoch [16/20], Loss: 0.0177\n",
            "Epoch [17/20], Loss: 0.0114\n",
            "Epoch [18/20], Loss: 0.0124\n",
            "Epoch [19/20], Loss: 0.0161\n",
            "Epoch [20/20], Loss: 0.0184\n",
            "Model saved to unet_model.pth\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# ------------------------------\n",
        "# 1. Simple U-Net-like Model (AICNet-inspired)\n",
        "# ------------------------------\n",
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(DoubleConv, self).__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=3):\n",
        "        super(UNet, self).__init__()\n",
        "        self.enc1 = DoubleConv(in_channels, 64)\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "        self.enc2 = DoubleConv(64, 128)\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "\n",
        "        self.bottleneck = DoubleConv(128, 256)\n",
        "\n",
        "        self.up2 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
        "        self.dec2 = DoubleConv(256, 128)\n",
        "        self.up1 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
        "        self.dec1 = DoubleConv(128, 64)\n",
        "\n",
        "        self.final = nn.Conv2d(64, out_channels, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        e1 = self.enc1(x)\n",
        "        e2 = self.enc2(self.pool1(e1))\n",
        "        b = self.bottleneck(self.pool2(e2))\n",
        "        d2 = self.up2(b)\n",
        "        d2 = torch.cat([d2, e2], dim=1)\n",
        "        d2 = self.dec2(d2)\n",
        "        d1 = self.up1(d2)\n",
        "        d1 = torch.cat([d1, e1], dim=1)\n",
        "        d1 = self.dec1(d1)\n",
        "        return self.final(d1)\n",
        "\n",
        "# ------------------------------\n",
        "# 2. Dataset with a Single Image Pair\n",
        "# ------------------------------\n",
        "class SingleImageDataset(Dataset):\n",
        "    def __init__(self, input_path, target_path, transform=None):\n",
        "        self.input_img = Image.open(input_path).convert(\"RGB\")\n",
        "        self.target_img = Image.open(target_path).convert(\"RGB\")\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return 1  # only one sample\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.transform:\n",
        "            input_tensor = self.transform(self.input_img)\n",
        "            target_tensor = self.transform(self.target_img)\n",
        "        else:\n",
        "            to_tensor = transforms.ToTensor()\n",
        "            input_tensor = to_tensor(self.input_img)\n",
        "            target_tensor = to_tensor(self.target_img)\n",
        "        return input_tensor, target_tensor\n",
        "\n",
        "# ------------------------------\n",
        "# 3. Training Script\n",
        "# ------------------------------\n",
        "def train_single_image(input_path, target_path, save_path=\"model.pth\", epochs=10):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((128, 128)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    dataset = SingleImageDataset(input_path, target_path, transform)\n",
        "    dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = UNet(in_channels=3, out_channels=3).to(device)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for inputs, targets in dataloader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "    print(f\"Model saved to {save_path}\")\n",
        "\n",
        "# ------------------------------\n",
        "# 4. Run Training\n",
        "# ------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Replace these with your image paths\n",
        "    input_image_path = \"images.jpeg\"\n",
        "    target_image_path = \"images.jpeg\"\n",
        "\n",
        "    if not os.path.exists(input_image_path) or not os.path.exists(target_image_path):\n",
        "        print(\"‚ùå Please put your input.jpg and target.jpg in this folder first!\")\n",
        "    else:\n",
        "        train_single_image(input_image_path, target_image_path, save_path=\"unet_model.pth\", epochs=20)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# import the UNet class from your training script\n",
        "from train_unet import UNet   # üëà change if your file name is different\n",
        "\n",
        "# initialize the model\n",
        "model = UNet(in_channels=3, out_channels=3)\n",
        "\n",
        "# load trained weights\n",
        "model.load_state_dict(torch.load(\"unet_model.pth\", map_location=\"cpu\"))\n",
        "model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATc8Q4ibiNFE",
        "outputId": "759124d0-2b15-450e-fc54-28ea1ff71b30"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "UNet(\n",
              "  (enc1): DoubleConv(\n",
              "    (block): Sequential(\n",
              "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): ReLU(inplace=True)\n",
              "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (3): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (enc2): DoubleConv(\n",
              "    (block): Sequential(\n",
              "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): ReLU(inplace=True)\n",
              "      (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (3): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (bottleneck): DoubleConv(\n",
              "    (block): Sequential(\n",
              "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): ReLU(inplace=True)\n",
              "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (3): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (up2): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
              "  (dec2): DoubleConv(\n",
              "    (block): Sequential(\n",
              "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): ReLU(inplace=True)\n",
              "      (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (3): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (up1): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
              "  (dec1): DoubleConv(\n",
              "    (block): Sequential(\n",
              "      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): ReLU(inplace=True)\n",
              "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (3): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (final): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_input = torch.randn(1, 3, 128, 128)  # (batch=1, channels=3, H=128, W=128)\n"
      ],
      "metadata": {
        "id": "cdKCej6siNHY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx onnxruntime\n",
        "\n",
        "torch.onnx.export(\n",
        "    model,                         # model\n",
        "    dummy_input,                   # dummy input\n",
        "    \"unet_model.onnx\",             # output file name\n",
        "    export_params=True,            # store trained weights inside ONNX file\n",
        "    opset_version=11,              # ONNX opset version (11 is widely supported)\n",
        "    do_constant_folding=True,      # optimize constant folding\n",
        "    input_names=['input'],         # input tensor name\n",
        "    output_names=['output'],       # output tensor name\n",
        "    dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
        ")\n",
        "print(\"‚úÖ Exported to unet_model.onnx\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4VNYB9OiNKv",
        "outputId": "bd6c17a6-d366-4534-89a9-593695e79c39"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.19.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (7.0 kB)\n",
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.22.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.12/dist-packages (from onnx) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from onnx) (5.29.5)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.12/dist-packages (from onnx) (4.15.0)\n",
            "Requirement already satisfied: ml_dtypes in /usr/local/lib/python3.12/dist-packages (from onnx) (0.5.3)\n",
            "Collecting coloredlogs (from onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (25.2.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (25.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (1.13.3)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Downloading onnx-1.19.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (18.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.22.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: humanfriendly, onnx, coloredlogs, onnxruntime\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnx-1.19.0 onnxruntime-1.22.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3996665406.py:3: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
            "  torch.onnx.export(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Exported to unet_model.onnx\n"
          ]
        }
      ]
    }
  ]
}